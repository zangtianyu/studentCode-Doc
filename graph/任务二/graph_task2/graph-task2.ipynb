{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv, GCNConv, SAGEConv, GINConv\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        r = (src * dst).sum(dim=-1)\n",
    "        return r\n",
    "\n",
    "    def forward(self, data, edge_label_index):\n",
    "        z = self.encode(data)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        r = (src * dst).sum(dim=-1)\n",
    "        return r\n",
    "\n",
    "    def forward(self, data, edge_label_index):\n",
    "        z = self.encode(data)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=8, concat=False)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels, heads=8, concat=False)\n",
    "\n",
    "    def encode(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        r = (src * dst).sum(dim=-1)\n",
    "        return r\n",
    "\n",
    "    def forward(self, data, edge_label_index):\n",
    "        z = self.encode(data)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(Linear(in_channels, 64), ReLU(), Linear(64, 64)))\n",
    "        self.conv2 = GINConv(Sequential(Linear(64, 64), ReLU(), Linear(64, out_channels)))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        r = (src * dst).sum(dim=-1)\n",
    "        return r\n",
    "\n",
    "    def forward(self, data, edge_label_index):\n",
    "        z = self.encode(data)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from pytorchtools import EarlyStopping\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_metrics(out, edge_label):\n",
    "    edge_label = edge_label.cpu().numpy()\n",
    "    out = out.cpu().numpy()\n",
    "    pred = (out > 0.5).astype(int)\n",
    "    auc = roc_auc_score(edge_label, out)\n",
    "    f1 = f1_score(edge_label, pred)\n",
    "    ap = average_precision_score(edge_label, out)\n",
    "\n",
    "    return auc, f1, ap\n",
    "\n",
    "\n",
    "def save_pickle(dataset, file_name):\n",
    "    f = open(file_name, \"wb\")\n",
    "    pickle.dump(dataset, f, protocol=4)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    f = open(file_name, \"rb+\")\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train_negative_sample(train_data):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_data = train_data.to(device)\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    return edge_label, edge_label_index\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, val_data, test_data):\n",
    "    model.eval()\n",
    "    # cal val loss\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "    #val_data = val_data.to(device)\n",
    "    out = model(val_data,\n",
    "                val_data.edge_label_index).view(-1)\n",
    "    val_loss = criterion(out, val_data.edge_label)\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    out = model(test_data,\n",
    "                test_data.edge_label_index.to(device)).view(-1).sigmoid()\n",
    "    model.train()\n",
    "\n",
    "    auc, f1, ap = get_metrics(out, test_data.edge_label)\n",
    "\n",
    "    return val_loss, auc, ap\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, test_data, save_model_path):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    min_epochs = 10\n",
    "    min_val_loss = np.Inf\n",
    "    final_test_auc = 0\n",
    "    final_test_ap = 0\n",
    "    best_model = None\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(10)):\n",
    "        optimizer.zero_grad()\n",
    "        edge_label, edge_label_index = train_negative_sample(train_data)\n",
    "        out = model(train_data, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validation\n",
    "        val_loss, test_auc, test_ap = test(model, val_data, test_data)\n",
    "        if epoch + 1 > min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            final_test_auc = test_auc\n",
    "            final_test_ap = test_ap\n",
    "            #best_model = copy.deepcopy(model)\n",
    "            # save model\n",
    "            #state = {'model': best_model.state_dict()}\n",
    "            #torch.save(state, save_model_path)\n",
    "\n",
    "        # scheduler.step()\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.4f} test_auc {:.4f} test_ap {:.4f}'\n",
    "              .format(epoch, loss.item(), val_loss, test_auc, test_ap))\n",
    "\n",
    "    #state = {'model': best_model.state_dict()}\n",
    "    #torch.save(state, save_model_path)\n",
    "\n",
    "    return final_test_auc, final_test_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "dataset = Planetoid('./dataset', name='CiteSeer', transform=transform)\n",
    "# dataset = Planetoid('./dataset', name='Cora', transform=transform)\n",
    "# dataset = Flickr(root='./dataset', name='Flickr', transform=transform)\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "train_data, val_data, test_data = train_data.to(device), val_data.to(device), test_data.to(device)\n",
    "\n",
    "model = GCN(dataset.num_features, 64, 128).to(device)\n",
    "# model = GAT(dataset.num_features, 64, 128).to(device)\n",
    "# model = SAGE(dataset.num_features, 64, 128).to(device)\n",
    "# model = GIN(dataset.num_features, 64, 128).to(device)\n",
    "train(model, train_data, val_data, test_data, device)\n",
    "test(model, test_data, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
