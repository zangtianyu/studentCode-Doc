#!/usr/bin/env python3
"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯æ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import *
from data.data_loader import DataLoader
from data.dataset import create_data_loaders
from utils.text_processor import TextProcessor
from utils.vocabulary import Vocabulary
from embeddings.embedding_loader import create_embedding_from_config
from models.cnn_model import CNNTextClassifier
from models.rnn_model import RNNTextClassifier
from training.trainer import Trainer

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("1. æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
        text_processor = TextProcessor()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = DataLoader(
            data_dir=DATA_DIR,
            train_file=TRAIN_FILE,
            test_file=TEST_FILE,
            text_processor=text_processor
        )
        
        # åŠ è½½æ•°æ®
        texts, labels, _, _ = data_loader.load_data()
        
        print(f"   æˆåŠŸåŠ è½½ {len(texts)} æ¡æ–‡æœ¬æ•°æ®")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
        
        # æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†
        sample_text = texts[0] if texts else "This is a test sentence."
        processed = text_processor.process_text(sample_text)
        print(f"   åŸå§‹æ–‡æœ¬: {sample_text[:50]}...")
        print(f"   å¤„ç†å: {processed[:10]}")
        
        return True
        
    except Exception as e:
        print(f"   æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_vocabulary():
    """æµ‹è¯•è¯æ±‡è¡¨æ„å»º"""
    print("2. æµ‹è¯•è¯æ±‡è¡¨æ„å»º...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_texts = [
            ["this", "is", "a", "test"],
            ["another", "test", "sentence"],
            ["test", "vocabulary", "building"]
        ]
        
        # æ„å»ºè¯æ±‡è¡¨
        vocab = Vocabulary(max_size=100, min_freq=1)
        vocab.build_from_texts(test_texts)
        
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print(f"   æµ‹è¯•ç¼–ç : {vocab.encode(['this', 'is', 'unknown'])}")
        
        return True
        
    except Exception as e:
        print(f"   è¯æ±‡è¡¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_embedding():
    """æµ‹è¯•è¯åµŒå…¥"""
    print("3. æµ‹è¯•è¯åµŒå…¥...")
    
    try:
        # åˆ›å»ºç®€å•è¯æ±‡è¡¨
        vocab = Vocabulary(max_size=100, min_freq=1)
        vocab.build_from_texts([["test", "word", "embedding"]])
        
        # æµ‹è¯•éšæœºåµŒå…¥
        embedding_config = EMBEDDING_CONFIGS['random']
        embedding_layer = create_embedding_from_config(
            vocabulary=vocab,
            config=embedding_config,
            embedding_dim=50  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦è¿›è¡Œæµ‹è¯•
        )
        
        print(f"   åµŒå…¥å±‚å½¢çŠ¶: {embedding_layer.weight.shape}")
        print(f"   åµŒå…¥å±‚å¯è®­ç»ƒ: {embedding_layer.weight.requires_grad}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.tensor([[1, 2, 0]])  # æ‰¹æ¬¡å¤§å°1ï¼Œåºåˆ—é•¿åº¦3
        output = embedding_layer(test_input)
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"   è¯åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_models():
    """æµ‹è¯•æ¨¡å‹åˆ›å»ºå’Œå‰å‘ä¼ æ’­"""
    print("4. æµ‹è¯•æ¨¡å‹...")
    
    try:
        # æ¨¡å‹å‚æ•°
        vocab_size = 100
        embedding_dim = 50
        num_classes = 2
        
        # æµ‹è¯•CNNæ¨¡å‹
        print("   æµ‹è¯•CNNæ¨¡å‹...")
        cnn_model = CNNTextClassifier(
            vocab_size=vocab_size,
            embedding_dim=embedding_dim,
            num_classes=num_classes,
            num_filters=10,
            filter_sizes=[2, 3],
            dropout=0.1
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randint(0, vocab_size, (2, 10))  # æ‰¹æ¬¡å¤§å°2ï¼Œåºåˆ—é•¿åº¦10
        cnn_output = cnn_model(test_input)
        print(f"   CNNè¾“å‡ºå½¢çŠ¶: {cnn_output.shape}")
        
        # æµ‹è¯•RNNæ¨¡å‹
        print("   æµ‹è¯•RNNæ¨¡å‹...")
        rnn_model = RNNTextClassifier(
            vocab_size=vocab_size,
            embedding_dim=embedding_dim,
            num_classes=num_classes,
            hidden_dim=32,
            num_layers=1,
            dropout=0.1
        )
        
        rnn_output = rnn_model(test_input)
        print(f"   RNNè¾“å‡ºå½¢çŠ¶: {rnn_output.shape}")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        cnn_model.print_model_info()
        
        return True
        
    except Exception as e:
        print(f"   æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_training_setup():
    """æµ‹è¯•è®­ç»ƒè®¾ç½®"""
    print("5. æµ‹è¯•è®­ç»ƒè®¾ç½®...")
    
    try:
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        batch_size = 4
        seq_len = 8
        vocab_size = 50
        num_classes = 2
        
        # åˆ›å»ºæ¨¡å‹
        model = CNNTextClassifier(
            vocab_size=vocab_size,
            embedding_dim=32,
            num_classes=num_classes,
            num_filters=8,
            filter_sizes=[2, 3],
            dropout=0.1
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_sequences = [
            [1, 2, 3, 4, 5, 0, 0, 0],
            [2, 3, 4, 5, 6, 7, 0, 0],
            [1, 3, 5, 7, 9, 11, 13, 0],
            [2, 4, 6, 8, 10, 12, 14, 16]
        ]
        test_labels = [0, 1, 0, 1]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        from data.dataset import TextClassificationDataset
        from torch.utils.data import DataLoader
        
        dataset = TextClassificationDataset(test_sequences, test_labels)
        data_loader = DataLoader(dataset, batch_size=2, shuffle=False)
        
        # æµ‹è¯•ä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        model.train()
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        for inputs, targets in data_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            print(f"   è®­ç»ƒæ­¥éª¤æˆåŠŸï¼ŒæŸå¤±: {loss.item():.4f}")
            break  # åªæµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        
        return True
        
    except Exception as e:
        print(f"   è®­ç»ƒè®¾ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆä½¿ç”¨å°æ•°æ®é›†ï¼‰"""
    print("6. æµ‹è¯•å®Œæ•´æµç¨‹...")
    
    try:
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        train_path = os.path.join(DATA_DIR, TRAIN_FILE)
        if not os.path.exists(train_path):
            print(f"   æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_path}")
            print("   è·³è¿‡å®Œæ•´æµç¨‹æµ‹è¯•")
            return True
        
        # åˆ›å»ºå®éªŒè¿è¡Œå™¨
        from experiments.run_experiments import ExperimentRunner
        
        runner = ExperimentRunner()
        
        # å‡†å¤‡æ•°æ®ï¼ˆåªä½¿ç”¨å°éƒ¨åˆ†æ•°æ®è¿›è¡Œæµ‹è¯•ï¼‰
        print("   å‡†å¤‡æ•°æ®...")
        runner.prepare_data()
        
        # åˆ›å»ºå°æ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        embedding_config = EMBEDDING_CONFIGS['random']
        model = runner.create_model('cnn', embedding_config, {
            'num_filters': 8,
            'filter_sizes': [2, 3],
            'dropout': 0.1
        })
        
        print("   æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•è®­ç»ƒé…ç½®
        test_config = TRAINING_CONFIG.copy()
        test_config.update({
            'epochs': 1,  # åªè®­ç»ƒ1ä¸ªepochè¿›è¡Œæµ‹è¯•
            'batch_size': 16,
            'learning_rate': 0.001
        })
        
        print("   å¼€å§‹æµ‹è¯•è®­ç»ƒ...")
        result = runner.train_single_model(model, "æµ‹è¯•æ¨¡å‹", test_config)
        
        print(f"   æµ‹è¯•è®­ç»ƒå®Œæˆ!")
        print(f"   éªŒè¯å‡†ç¡®ç‡: {result['training_result']['best_val_accuracy']:.4f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {result['test_metrics']['accuracy']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"   å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("æ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿæµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    create_directories()
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        test_data_loading,
        test_vocabulary,
        test_embedding,
        test_models,
        test_training_setup,
        test_full_pipeline
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
                print("   âœ“ é€šè¿‡\n")
            else:
                print("   âœ— å¤±è´¥\n")
        except Exception as e:
            print(f"   âœ— å¼‚å¸¸: {e}\n")
    
    print("=" * 40)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        print("\nå¯ä»¥å¼€å§‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:")
        print("python main.py --model cnn --embedding random --epochs 5")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")
    
    return passed == total

if __name__ == "__main__":
    main()
