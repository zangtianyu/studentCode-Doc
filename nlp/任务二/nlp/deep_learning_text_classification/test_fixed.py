#!/usr/bin/env python3
"""
ä¿®å¤åçš„æµ‹è¯•è„šæœ¬ - åªæµ‹è¯•éšæœºåµŒå…¥
"""

import os
import sys
import torch
from config import *
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def main():
    """è¿è¡Œä¿®å¤åçš„æµ‹è¯•"""
    print("ä¿®å¤åçš„æ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å¯¼å…¥æ¨¡å—
       
        from data.data_loader import DataLoader
        from data.dataset import TextDataset, create_data_loaders
        from models.cnn_model import CNNTextClassifier
        from training.trainer import Trainer
        from utils.text_processor import TextProcessor
        from utils.vocabulary import Vocabulary
        
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 1. æ•°æ®å‡†å¤‡
        print("\n1. å‡†å¤‡æ•°æ®...")
        text_processor = TextProcessor()
        data_loader = DataLoader(
            data_dir="../data",
            train_file="train.tsv/train.tsv",
            test_file="test.tsv/test.tsv",
            text_processor=text_processor
        )
        
        # åŠ è½½å°‘é‡æ•°æ®
        texts, labels, _, _ = data_loader.load_data()
        
        # åªä½¿ç”¨å‰1000ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        texts = texts[:1000]
        labels = labels[:1000]
        
        print(f"ä½¿ç”¨ {len(texts)} ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•")
        
        # ç®€å•åˆ†å‰²æ•°æ®
        from sklearn.model_selection import train_test_split
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42
        )
        
        # é¢„å¤„ç†
        train_texts = data_loader.preprocess_texts(train_texts)
        val_texts = data_loader.preprocess_texts(val_texts)
        
        # æ„å»ºè¯æ±‡è¡¨
        vocabulary = Vocabulary(max_size=2000, min_freq=1)
        vocabulary.build_from_texts(train_texts)
        print(f"è¯æ±‡è¡¨å¤§å°: {len(vocabulary)}")
        
        # è½¬æ¢ä¸ºåºåˆ—
        train_sequences = [vocabulary.encode(text) for text in train_texts]
        val_sequences = [vocabulary.encode(text) for text in val_texts]
        
        # å¡«å……åºåˆ—
        max_length = 30  # å¾ˆçŸ­çš„åºåˆ—
        train_sequences = data_loader.pad_sequences(train_sequences, max_length)
        val_sequences = data_loader.pad_sequences(val_sequences, max_length)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TextDataset(train_sequences, train_labels)
        val_dataset = TextDataset(val_sequences, val_labels)
        
        train_loader, val_loader, _ = create_data_loaders(
            train_dataset, val_dataset, val_dataset,
            batch_size=16, eval_batch_size=32
        )
        
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        
        # 2. åˆ›å»ºå°æ¨¡å‹
        print("\n2. åˆ›å»ºCNNæ¨¡å‹...")
        model = CNNTextClassifier(
            vocab_size=len(vocabulary),
            embedding_dim=32,   # å¾ˆå°çš„åµŒå…¥ç»´åº¦
            num_classes=2,
            num_filters=16,     # å¾ˆå°‘çš„è¿‡æ»¤å™¨
            filter_sizes=[2, 3],
            dropout=0.2
        )
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
        print("âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
        
        # 3. è®­ç»ƒ
        print("\n3. å¼€å§‹è®­ç»ƒ (1ä¸ªepoch)...")
        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=val_loader,
            device=device,
            config={
                'epochs': 1,  # åªè®­ç»ƒ1ä¸ªepoch
                'learning_rate': 0.001,
                'optimizer': 'Adam',
                'early_stopping_patience': 10,
                'gradient_clip': 1.0,
                'save_best_model': False,
                'log_interval': 10
            }
        )
        
        # è®­ç»ƒæ¨¡å‹
        result = trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆ")
        
        # 4. è¯„ä¼°
        print("\n4. è¯„ä¼°æ¨¡å‹...")
        metrics = trainer.evaluate()
        
        print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸ!")
        print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        print(f"F1åˆ†æ•°: {metrics['f1_score']:.4f}")
        print(f"è®­ç»ƒæ—¶é—´: {result['total_time']:.2f}ç§’")
        
        print("\nâœ… æ‰€æœ‰é—®é¢˜å·²ä¿®å¤ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸!")
        print("\næ¨èçš„ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œå®Œæ•´çš„CNNè®­ç»ƒ:")
        print("   python main.py --model cnn --embedding random --epochs 10")
        print("\n2. å¦‚æœæƒ³ä½¿ç”¨GloVeåµŒå…¥ï¼Œå…ˆè§£å‹æ–‡ä»¶:")
        print("   python extract_glove.py")
        print("   python main.py --model cnn --embedding glove --epochs 10")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()
