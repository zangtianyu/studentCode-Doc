#!/usr/bin/env python3
"""
ç®€åŒ–æµ‹è¯•è„šæœ¬ - åªæµ‹è¯•éšæœºåµŒå…¥ï¼Œé¿å…GloVeä¸‹è½½é—®é¢˜
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import *
from data.data_loader import DataLoader
from data.dataset import TextDataset, create_data_loaders
from models.cnn_model import CNNTextClassifier
from models.rnn_model import RNNTextClassifier
from training.trainer import Trainer
from utils.text_processor import TextProcessor
from utils.vocabulary import Vocabulary
from utils.metrics import MetricsCalculator

def test_single_model(model_type='cnn', epochs=3):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"æµ‹è¯• {model_type.upper()} æ¨¡å‹ ({epochs} epochs)")
    print("=" * 50)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. å‡†å¤‡æ•°æ®
        print("1. å‡†å¤‡æ•°æ®...")
        text_processor = TextProcessor()
        data_loader = DataLoader(
            data_dir="../data",
            train_file="train.tsv/train.tsv",
            test_file="test.tsv/test.tsv",
            text_processor=text_processor
        )
        
        # åŠ è½½æ•°æ®
        texts, labels, test_texts, test_labels = data_loader.load_data()
        
        # é™åˆ¶æ•°æ®é‡ä»¥åŠ å¿«æµ‹è¯•
        max_samples = 10000
        if len(texts) > max_samples:
            texts = texts[:max_samples]
            labels = labels[:max_samples]
        
        print(f"è®­ç»ƒæ•°æ®: {len(texts)} æ ·æœ¬")
        
        # åˆ†å‰²æ•°æ®
        train_texts, train_labels, val_texts, val_labels = data_loader.split_data(
            texts, labels, test_size=0.2, random_state=42
        )
        
        # é¢„å¤„ç†
        train_texts = data_loader.preprocess_texts(train_texts)
        val_texts = data_loader.preprocess_texts(val_texts)
        
        # æ„å»ºè¯æ±‡è¡¨
        vocabulary = Vocabulary(max_size=10000, min_freq=2)
        vocabulary.build_from_texts(train_texts)
        print(f"è¯æ±‡è¡¨å¤§å°: {len(vocabulary)}")
        
        # è½¬æ¢ä¸ºåºåˆ—
        train_sequences = [vocabulary.encode(text) for text in train_texts]
        val_sequences = [vocabulary.encode(text) for text in val_texts]
        
        # å¡«å……åºåˆ—
        max_length = 100  # å‡å°‘åºåˆ—é•¿åº¦ä»¥åŠ å¿«è®­ç»ƒ
        train_sequences = data_loader.pad_sequences(train_sequences, max_length)
        val_sequences = data_loader.pad_sequences(val_sequences, max_length)
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = TextDataset(train_sequences, train_labels)
        val_dataset = TextDataset(val_sequences, val_labels)
        
        train_loader, val_loader, _ = create_data_loaders(
            train_dataset, val_dataset, val_dataset,
            batch_size=64, eval_batch_size=128
        )
        
        # 2. åˆ›å»ºæ¨¡å‹
        print("2. åˆ›å»ºæ¨¡å‹...")
        if model_type == 'cnn':
            model = CNNTextClassifier(
                vocab_size=len(vocabulary),
                embedding_dim=100,  # å‡å°‘åµŒå…¥ç»´åº¦
                num_classes=2,
                num_filters=64,     # å‡å°‘è¿‡æ»¤å™¨æ•°é‡
                filter_sizes=[2, 3, 4],
                dropout=0.5
            )
        else:  # rnn
            model = RNNTextClassifier(
                vocab_size=len(vocabulary),
                embedding_dim=100,  # å‡å°‘åµŒå…¥ç»´åº¦
                hidden_dim=64,      # å‡å°‘éšè—å±‚ç»´åº¦
                num_classes=2,
                num_layers=1,
                dropout=0.5,
                rnn_type='LSTM'
            )
        
        model.print_model_info()
        
        # 3. è®­ç»ƒæ¨¡å‹
        print("3. å¼€å§‹è®­ç»ƒ...")
        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=val_loader,  # ä½¿ç”¨éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†
            device=device,
            config={
                'epochs': epochs,
                'learning_rate': 0.001,
                'optimizer': 'Adam',
                'scheduler': None,
                'early_stopping_patience': 3,
                'gradient_clip': 1.0,
                'save_best_model': False,  # ä¸ä¿å­˜æ¨¡å‹
                'log_interval': 50
            }
        )
        
        # è®­ç»ƒ
        training_result = trainer.train()
        
        # 4. è¯„ä¼°
        print("4. è¯„ä¼°æ¨¡å‹...")
        test_metrics = trainer.evaluate()
        
        print(f"\n{model_type.upper()} æ¨¡å‹æµ‹è¯•ç»“æœ:")
        print(f"å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        print(f"F1åˆ†æ•°: {test_metrics['f1_score']:.4f}")
        print(f"è®­ç»ƒæ—¶é—´: {training_result['total_time']:.2f}ç§’")
        
        return True
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("æ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±» - ç®€åŒ–æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•CNNæ¨¡å‹
    success_cnn = test_single_model('cnn', epochs=2)
    
    print("\n" + "=" * 60)
    
    # æµ‹è¯•RNNæ¨¡å‹
    success_rnn = test_single_model('rnn', epochs=2)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“:")
    print(f"CNNæ¨¡å‹: {'âœ“ æˆåŠŸ' if success_cnn else 'âœ— å¤±è´¥'}")
    print(f"RNNæ¨¡å‹: {'âœ“ æˆåŠŸ' if success_rnn else 'âœ— å¤±è´¥'}")
    
    if success_cnn or success_rnn:
        print("\nğŸ‰ è‡³å°‘ä¸€ä¸ªæ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        print("ç³»ç»ŸåŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒ:")
        print("python main.py --model cnn --embedding random --epochs 10")
    else:
        print("\nâš ï¸ æ‰€æœ‰æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

if __name__ == "__main__":
    main()
